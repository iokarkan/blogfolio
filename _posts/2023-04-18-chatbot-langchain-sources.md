---
layout: post
title: "Create Chatbots with a customizable knowledge base using LangChain"
tags: generative AI LLM langchain
icon: "fas fa-hammer"
---

[LangChain](https://python.langchain.com/en/latest/index.html) is an innovative library that simplifies the creation of
chatbots. One of its key features that I've been exploring is the ability to store and retrieve text data as knowledge
sources. After playing with it for a while, I've come to think that this may be one of the first big corporate utility
applications of Large Language Models (LLMs). This post is an overview of how to create a chatbot using LangChain
that leverages this novel functionality.

<div class="m-auto rounded-2xl bg-slate-200 text-center ">
    <p class="p-2 text-sm text-indigo-800 font-mono">Check out a demo with the full <a
            href="https://huggingface.co/spaces/ioanniskarkanias/chatbot-with-sources/blob/main/app.py">code</a> in my
        HuggingFace <a href="https://huggingface.co/spaces/ioanniskarkanias/chatbot-with-sources">space</a>
        <br />
        <br />
        or watch me using it:
        <iframe class='m-auto w-[100%] px-8 py-2' height='300' allowfullscreen="allowfullscreen"
        mozallowfullscreen="mozallowfullscreen" 
        msallowfullscreen="msallowfullscreen" 
        oallowfullscreen="oallowfullscreen" 
        webkitallowfullscreen="webkitallowfullscreen" src="https://youtube.com/embed/rKbipz38CkA">
        </iframe>
    </p>

</div>

## Customizable Knowledge Sources

Over the years, vector databases such as [Pinecone](https://www.pinecone.io/) and [Chroma
DB](https://www.trychroma.com/) have emerged as powerful tools for managing and searching large-scale embeddings (a.k.a.
numerical representations of text data) generated by LLMs. By utilizing such vector databases,
developers can efficiently store and retrieve embeddings, enabling them to build scalable applications for real-time and
resource-intensive tasks such as natural language processing, recommendation systems, and semantic search.

In our case, we want to transition into the era of retrieving internal information using natural language. Langchain
allows to create a chatbot to access and use this information when answering questions, providing more accurate and
context-specific responses.

At initialization of the app, the chatbot is context-free, and can act as a Conversational model. In the
`ChatbotBackend` class, the `update_sources` method is responsible for adding the uploaded text data to the chatbot's
knowledge database. It first loads the data and splits it into smaller chunks, then adds the chunks to the database and
updates the chatbot's retrieval chain. A "chain" in the LangChain library refers to a series of connected components
that work together to process user input and generate a response using a Large Language Model and optional retrieval
mechanisms.

```
def update_sources(self, fname):
    if Path(fname).suffix==".txt":
        loader = TextLoader(file_path=f"{fname}")
        data = loader.load()        
    ...

    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    texts = text_splitter.split_documents(data)

    if not self.db:
        self.db = Chroma.from_documents(texts, self.embeddings)
    else:
        self.db.add_documents(texts)
    
    self.update_chain()
```

The `update_chain` method swaps the chatbot's chain to a `ConversationalRetrievalChain` once sources are added. This
ensures that the chatbot utilizes the knowledge from the uploaded sources when generating responses. Importantly, the similarity search `k` parameter
should be updated to match the number of sources to be queried, so that all possible matches can be returned.

```
def update_chain(self):
    self.similarity_k = len(self.sources)
    retriever = self.db.as_retriever(search_kwargs={"k": self.similarity_k})
    ...
```

## User Interface for Knowledge Source Management

The Gradio interface allows users to easily manage the chatbot's knowledge sources. They can upload .txt or .csv files
containing text data, which will be added to the chatbot's knowledge database.

```
btn.upload(fn=add_file,
    inputs=[chatbot_output, btn, chatbot_instance, markdown],
    outputs=[chatbot_output, chatbot_instance, markdown])
```

Gradio provides the `gradio.State()` which is support for [session state](https://gradio.app/state-in-blocks/) in your
applications. This is necessary to de-couple one user's experiences from another's. In our examples, each user has their
own "database" to query, but in a professional environment the knowledgebase exposed to users may be common, to some
extent.

Once a file is uploaded, the chatbot's knowledge sources are updated, and the user can query information related to the
uploaded content. Uploaded files are displayed in a markdown panel, providing a clear overview of the chatbot's current
knowledge sources.

## Enhanced Chatbot Responses

When the chatbot receives a question, it will consider both its pre-trained knowledge and the uploaded text data to
generate a response.

```
def generate_response(self, user_input, chat_history=None):
    ...
```

The `generate_response` method handles this process, utilizing the `ConversationalRetrievalChain` when querying the
chatbot's knowledge database to generate more accurate and context-specific answers.

## External Sources

LangChain also provides easy-to-use utilities for accessing external information, such as Wikipedia. It also allows an
application to use calculators, or even a Python console. This follows the recent introduction of plug-ins to commercial
interfaces such as ChatGPT (with GPT-4). Stay connected and you'll be able to check this out here soon!